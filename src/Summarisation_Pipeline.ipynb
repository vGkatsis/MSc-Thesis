{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Summarisation Model results from json file.\n",
    "\n",
    "with open(\"../data/Model_Results/summarisation_results.jsonl\", 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "    \n",
    "summarisationResults = {}\n",
    "for json_str in json_list:\n",
    "    \n",
    "    result = json.loads(json_str)\n",
    "    summaryList = sorted(result[\"sentence_scores\"], key=lambda x:x[1], reverse=True)[:4]\n",
    "    summary = \"\"\n",
    "    for sentence in summaryList:\n",
    "        summary += sentence[0] + ' '\n",
    "    \n",
    "    summarisationResults[result[\"id\"]] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisationData = pd.DataFrame(data=summarisationResults, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarisationDataDict = {}\n",
    "\n",
    "for json_id in summarisationData.columns:\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    features[\"summary\"] = summarisationData[json_id].values[0]\n",
    "    \n",
    "    summarisationDataDict[json_id] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11972.json</th>\n",
       "      <th>11685.json</th>\n",
       "      <th>11096.json</th>\n",
       "      <th>5209.json</th>\n",
       "      <th>9524.json</th>\n",
       "      <th>5962.json</th>\n",
       "      <th>7070.json</th>\n",
       "      <th>1046.json</th>\n",
       "      <th>12849.json</th>\n",
       "      <th>13270.json</th>\n",
       "      <th>...</th>\n",
       "      <th>11576.json</th>\n",
       "      <th>3461.json</th>\n",
       "      <th>9464.json</th>\n",
       "      <th>10227.json</th>\n",
       "      <th>11707.json</th>\n",
       "      <th>3425.json</th>\n",
       "      <th>2977.json</th>\n",
       "      <th>294.json</th>\n",
       "      <th>3580.json</th>\n",
       "      <th>8384.json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>\"Building a wall\" on the border \"will take lit...</td>\n",
       "      <td>Those numbers show that as of October 2015, th...</td>\n",
       "      <td>If he did, he would have known that Senator Mc...</td>\n",
       "      <td>\"…She supports taking $500 billion away from M...</td>\n",
       "      <td>Scott Walker helped run a \"criminal scheme\" to...</td>\n",
       "      <td>The campaign accurately quoted a figure whose ...</td>\n",
       "      <td>So $30-31 million per year would, in fact, be ...</td>\n",
       "      <td>The Obama administration has emphasized many o...</td>\n",
       "      <td>\"It matters who’s leading the country, and it ...</td>\n",
       "      <td>Pence described the donors as major. \"The nati...</td>\n",
       "      <td>...</td>\n",
       "      <td>We’re not sure Sanders made that entirely clea...</td>\n",
       "      <td>The $20 million designated for Cuba \"focuses o...</td>\n",
       "      <td>There also are no Asian or Pacific Islander Re...</td>\n",
       "      <td>\"The United States is in the longest stretch o...</td>\n",
       "      <td>\"Secretary Clinton changes her position on thi...</td>\n",
       "      <td>Under the header \"New jobs created by the Stre...</td>\n",
       "      <td>They said low pay, increased work demands and ...</td>\n",
       "      <td>• Comstock, an adviser and frequent spokeswoma...</td>\n",
       "      <td>\"House Republicans under Paul Ryan's leadershi...</td>\n",
       "      <td>\"I will work in a bipartisan way to get it don...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                11972.json  \\\n",
       "summary  \"Building a wall\" on the border \"will take lit...   \n",
       "\n",
       "                                                11685.json  \\\n",
       "summary  Those numbers show that as of October 2015, th...   \n",
       "\n",
       "                                                11096.json  \\\n",
       "summary  If he did, he would have known that Senator Mc...   \n",
       "\n",
       "                                                 5209.json  \\\n",
       "summary  \"…She supports taking $500 billion away from M...   \n",
       "\n",
       "                                                 9524.json  \\\n",
       "summary  Scott Walker helped run a \"criminal scheme\" to...   \n",
       "\n",
       "                                                 5962.json  \\\n",
       "summary  The campaign accurately quoted a figure whose ...   \n",
       "\n",
       "                                                 7070.json  \\\n",
       "summary  So $30-31 million per year would, in fact, be ...   \n",
       "\n",
       "                                                 1046.json  \\\n",
       "summary  The Obama administration has emphasized many o...   \n",
       "\n",
       "                                                12849.json  \\\n",
       "summary  \"It matters who’s leading the country, and it ...   \n",
       "\n",
       "                                                13270.json  ...  \\\n",
       "summary  Pence described the donors as major. \"The nati...  ...   \n",
       "\n",
       "                                                11576.json  \\\n",
       "summary  We’re not sure Sanders made that entirely clea...   \n",
       "\n",
       "                                                 3461.json  \\\n",
       "summary  The $20 million designated for Cuba \"focuses o...   \n",
       "\n",
       "                                                 9464.json  \\\n",
       "summary  There also are no Asian or Pacific Islander Re...   \n",
       "\n",
       "                                                10227.json  \\\n",
       "summary  \"The United States is in the longest stretch o...   \n",
       "\n",
       "                                                11707.json  \\\n",
       "summary  \"Secretary Clinton changes her position on thi...   \n",
       "\n",
       "                                                 3425.json  \\\n",
       "summary  Under the header \"New jobs created by the Stre...   \n",
       "\n",
       "                                                 2977.json  \\\n",
       "summary  They said low pay, increased work demands and ...   \n",
       "\n",
       "                                                  294.json  \\\n",
       "summary  • Comstock, an adviser and frequent spokeswoma...   \n",
       "\n",
       "                                                 3580.json  \\\n",
       "summary  \"House Republicans under Paul Ryan's leadershi...   \n",
       "\n",
       "                                                 8384.json  \n",
       "summary  \"I will work in a bipartisan way to get it don...  \n",
       "\n",
       "[1 rows x 1400 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarisationData = pd.DataFrame().from_dict(data=summarisationDataDict)\n",
    "\n",
    "display(summarisationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Train_Eval_Test_Data/train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-962dc91253e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_trainDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'../data/Train_Eval_Test_Data/train.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_trainDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"json_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"justification_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_testDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'../data/Train_Eval_Test_Data/test.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_testDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"json_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"justification_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Train_Eval_Test_Data/train.tsv'"
     ]
    }
   ],
   "source": [
    "x_trainDf = pd.read_csv ('../data/Train_Eval_Test_Data/train.tsv', sep='\\t')\n",
    "x_trainDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "\n",
    "x_testDf = pd.read_csv ('../data/Train_Eval_Test_Data/test.tsv', sep='\\t')\n",
    "x_testDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "\n",
    "x_valDf = pd.read_csv ('../data/Train_Eval_Test_Data/val.tsv', sep='\\t')\n",
    "x_valDf.columns = [\"json_id\", \"claim\", \"justification_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainDf[\"summary\"] = \"\"\n",
    "x_testDf[\"summary\"]  = \"\"\n",
    "x_valDf[\"summary\"]   = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSummary(data, summaries):\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"json_id\"] in summaries.columns:\n",
    "            row[\"summary\"] = summaries[row[\"json_id\"]].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSummary(x_trainDf, summarisationData)\n",
    "addSummary(x_testDf, summarisationData)\n",
    "addSummary(x_valDf, summarisationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainDf = pd.DataFrame(data=x_trainDf[\"justification_label\"])\n",
    "y_testDf  = pd.DataFrame(data=x_testDf[\"justification_label\"])\n",
    "y_valDf   = pd.DataFrame(data=x_valDf[\"justification_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndStemm(sentence):\n",
    "    \n",
    "    sb = SnowballStemmer(\"english\")\n",
    "    \n",
    "    sentenceList = []\n",
    "    for token in sentence.split(' '):\n",
    "        \n",
    "        token = token.lower()\n",
    "        token = sb.stem(token)\n",
    "        \n",
    "        sentenceList.append(token)\n",
    "        \n",
    "    newSentence = ' '.join(sentenceList)\n",
    "    return newSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list containing claims + attention explanations\n",
    "\n",
    "trainClaimSumm  = []\n",
    "yTrainLabels   = y_trainDf['justification_label'].tolist()\n",
    "\n",
    "testClaimSumm   = []\n",
    "yTestLabels    = y_testDf['justification_label'].tolist()\n",
    "\n",
    "evalClaimSumm   = []\n",
    "yEvalLabels    = y_valDf['justification_label'].tolist()\n",
    "\n",
    "for index, row in x_trainDf.iterrows():\n",
    "    trainClaimSumm.append(cleanAndStemm(row['claim']) + cleanAndStemm(row['summary']))\n",
    "\n",
    "for index, row in x_testDf.iterrows():\n",
    "    testClaimSumm.append(cleanAndStemm(row['claim']) + cleanAndStemm(row['summary']))\n",
    "\n",
    "for index, row in x_valDf.iterrows():\n",
    "    evalClaimSumm.append(cleanAndStemm(row['claim']) + cleanAndStemm(row['summary']))\n",
    "    \n",
    "totalClaimSumm = trainClaimSumm + testClaimSumm + evalClaimSumm\n",
    "yTotalLabels = np.array(yTrainLabels + yTestLabels + yEvalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to number using Bag of Words (Bow)\n",
    "vectorizer = CountVectorizer(max_features=30)\n",
    "vectorizedTrain = vectorizer.fit_transform(trainClaimSumm).toarray()\n",
    "vectorizedTest  = vectorizer.fit_transform(testClaimSumm).toarray()\n",
    "vectorizedEval  = vectorizer.fit_transform(evalClaimSumm).toarray()\n",
    "vectorizedTotal = vectorizer.fit_transform(totalClaimSumm).toarray()\n",
    "\n",
    "# Convert Bow values according to TfIdf\n",
    "tfidfconverter = TfidfTransformer()\n",
    "XTrain = tfidfconverter.fit_transform(vectorizedTrain).toarray()\n",
    "XTest  = tfidfconverter.fit_transform(vectorizedTest).toarray()\n",
    "XEval  = tfidfconverter.fit_transform(vectorizedEval).toarray()\n",
    "XTotal = tfidfconverter.fit_transform(vectorizedTotal).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into a list of lists for the Word2Vec model\n",
    "\n",
    "w2vTrainClaimSum = []\n",
    "w2vTestClaimSum  = []\n",
    "w2vEvalClaimSum  = []\n",
    "\n",
    "for index, row in x_trainDf.iterrows():\n",
    "    w2vTrainClaimSum.append((row['claim'] + row['summary']).split(\" \"))\n",
    "\n",
    "for index, row in x_testDf.iterrows():\n",
    "    w2vTestClaimSum.append((row['claim'] + row['summary']).split(\" \"))\n",
    "\n",
    "for index, row in x_valDf.iterrows():\n",
    "    w2vEvalClaimSum.append((row['claim'] + row['summary']).split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-461-f86b847b1e84>:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
      "<ipython-input-461-f86b847b1e84>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "# Vectorize using Word2Vec\n",
    "\n",
    "# min_count = 3 & size = 40 & window=8 & sg=1 \n",
    "\n",
    "model = Word2Vec(sentences=w2vTrainClaimSum, min_count=3,size=40,workers=4, window=8, sg=1)\n",
    "\n",
    "words = set(model.wv.index2word)\n",
    "X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                         for ls in w2vTrainClaimSum])\n",
    "X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                         for ls in w2vTestClaimSum])\n",
    "\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TenFoldValidation_BowTfIdf(summarisationData, classifier):\n",
    "    \n",
    "    accuracyResults = np.zeros([10])\n",
    "    \n",
    "    for counter in range(10):\n",
    "        x_trainDf = pd.read_csv ('../data/Train_Eval_Test_Data/Iteration' + str(counter + 1) + '/train' + str(counter + 1) + '.tsv', sep='\\t')\n",
    "        x_trainDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "\n",
    "        x_testDf = pd.read_csv ('../data/Train_Eval_Test_Data/Iteration' + str(counter + 1) + '/test' + str(counter + 1) + '.tsv', sep='\\t')\n",
    "        x_testDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "        \n",
    "        x_trainDf[\"summary\"] = \"\"\n",
    "        x_testDf[\"summary\"]  = \"\"\n",
    "        \n",
    "        addSummary(x_trainDf, summarisationData)\n",
    "        addSummary(x_testDf, summarisationData)\n",
    "        \n",
    "        y_trainDf = pd.DataFrame(data=x_trainDf[\"justification_label\"])\n",
    "        y_testDf  = pd.DataFrame(data=x_testDf[\"justification_label\"])\n",
    "        \n",
    "        trainClaimSumm  = []\n",
    "        yTrainLabels   = y_trainDf['justification_label'].tolist()\n",
    "\n",
    "        testClaimSumm   = []\n",
    "        yTestLabels    = y_testDf['justification_label'].tolist()\n",
    "\n",
    "        for index, row in x_trainDf.iterrows():\n",
    "            trainClaimSumm.append(cleanAndStemm(row['claim']) + cleanAndStemm(row['summary']))\n",
    "\n",
    "        for index, row in x_testDf.iterrows():\n",
    "            testClaimSumm.append(cleanAndStemm(row['claim']) + cleanAndStemm(row['summary']))\n",
    "\n",
    "            \n",
    "        # Convert words to number using Bag of Words (Bow)\n",
    "        vectorizer = CountVectorizer(max_features=30)\n",
    "        vectorizedTrain = vectorizer.fit_transform(trainClaimSumm).toarray()\n",
    "        vectorizedTest  = vectorizer.fit_transform(testClaimSumm).toarray()\n",
    "\n",
    "        # Convert Bow values according to TfIdf\n",
    "        tfidfconverter = TfidfTransformer()\n",
    "        XTrain = tfidfconverter.fit_transform(vectorizedTrain).toarray()\n",
    "        XTest  = tfidfconverter.fit_transform(vectorizedTest).toarray()\n",
    "\n",
    "        classifier.fit(XTrain,yTrainLabels)\n",
    "\n",
    "        yPredLabels = classifier.predict(XTest)\n",
    "        \n",
    "        accuracyScore = accuracy_score(yTestLabels,yPredLabels)\n",
    "        accuracyResults[counter] = accuracyScore\n",
    "\n",
    "#         ConfusionMatrixDisplay.from_predictions(yTestLabels, yPredLabels, cmap='Greens')\n",
    "#         print(classification_report(yTestLabels,yPredLabels))\n",
    "        print(\"Iteration:\",counter+1,\" Accuracy Score: \",accuracyScore)\n",
    "    \n",
    "    return accuracyResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TenFoldValidation_Word2Vec(summarisationData, classifier):\n",
    "    \n",
    "    accuracyResults = np.zeros([10])\n",
    "\n",
    "    for counter in range(10):\n",
    "        x_trainDf = pd.read_csv ('../data/Train_Eval_Test_Data/Iteration' + str(counter + 1) + '/train' + str(counter + 1) + '.tsv', sep='\\t')\n",
    "        x_trainDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "\n",
    "        x_testDf = pd.read_csv ('../data/Train_Eval_Test_Data/Iteration' + str(counter + 1) + '/test' + str(counter + 1) + '.tsv', sep='\\t')\n",
    "        x_testDf.columns = [\"json_id\", \"claim\", \"justification_label\"]\n",
    "        \n",
    "        x_trainDf[\"summary\"] = \"\"\n",
    "        x_testDf[\"summary\"]  = \"\"\n",
    "        \n",
    "        addSummary(x_trainDf, summarisationData)\n",
    "        addSummary(x_testDf, summarisationData)\n",
    "        \n",
    "        y_trainDf = pd.DataFrame(data=x_trainDf[\"justification_label\"])\n",
    "        y_testDf  = pd.DataFrame(data=x_testDf[\"justification_label\"])\n",
    "        \n",
    "        yTrainLabels   = y_trainDf['justification_label'].tolist()\n",
    "        yTestLabels    = y_testDf['justification_label'].tolist()\n",
    "        \n",
    "        # Transform the data into a list of lists for the Word2Vec model\n",
    "        w2vTrainClaimSum = []\n",
    "        w2vTestClaimSum  = []\n",
    "\n",
    "        for index, row in x_trainDf.iterrows():\n",
    "            w2vTrainClaimSum.append((row['claim'] + row['summary']).split(\" \"))\n",
    "\n",
    "        for index, row in x_testDf.iterrows():\n",
    "            w2vTestClaimSum.append((row['claim'] + row['summary']).split(\" \"))\n",
    "\n",
    "            \n",
    "        # Vectorize using Word2Vec\n",
    "\n",
    "        # min_count = 3 & size = 40 & window=8 & sg=1 \n",
    "\n",
    "        model = Word2Vec(sentences=w2vTrainClaimSum, min_count=3,size=40,workers=4, window=8, sg=1)\n",
    "\n",
    "        words = set(model.wv.index2word)\n",
    "        X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                                 for ls in w2vTrainClaimSum])\n",
    "        X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])\n",
    "                                 for ls in w2vTestClaimSum])\n",
    "\n",
    "        X_train_vect_avg = []\n",
    "        for v in X_train_vect:\n",
    "            if v.size:\n",
    "                X_train_vect_avg.append(v.mean(axis=0))\n",
    "            else:\n",
    "                X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "\n",
    "        X_test_vect_avg = []\n",
    "        for v in X_test_vect:\n",
    "            if v.size:\n",
    "                X_test_vect_avg.append(v.mean(axis=0))\n",
    "            else:\n",
    "                X_test_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "        classifier.fit(X_train_vect_avg,yTrainLabels)\n",
    "\n",
    "        yPredLabels = classifier.predict(X_test_vect_avg)\n",
    "\n",
    "#         ConfusionMatrixDisplay.from_predictions(yTestLabels, yPredLabels, cmap='Greens')\n",
    "#         print(classification_report(yTestLabels,yPredLabels))\n",
    "\n",
    "        accuracyScore = accuracy_score(yTestLabels,yPredLabels)\n",
    "        accuracyResults[counter] = accuracyScore\n",
    "        \n",
    "        print(\"Iteration:\",counter+1,\" Accuracy Score: \",accuracyScore)\n",
    "        \n",
    "    return accuracyResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.4\n",
      "Iteration: 2  Accuracy Score:  0.4\n",
      "Iteration: 3  Accuracy Score:  0.4\n",
      "Iteration: 4  Accuracy Score:  0.5\n",
      "Iteration: 5  Accuracy Score:  0.4\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.4\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Dummy Classifier With Method Stratified\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "TenFoldValidation_BowTfIdf(summarisationData, dummy_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.4\n",
      "Iteration: 3  Accuracy Score:  0.5\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.2\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.2\n",
      "Iteration: 8  Accuracy Score:  0.2\n",
      "Iteration: 9  Accuracy Score:  0.3\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word 2 Vec\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Dummy Classifier With Method Stratified\n",
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "TenFoldValidation_Word2Vec(summarisationData, dummy_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Bow - Tf-Idf Results----\n",
      "\n",
      "Iteration: 1  Accuracy Score:  0.5\n",
      "Iteration: 2  Accuracy Score:  0.4\n",
      "Iteration: 3  Accuracy Score:  0.1\n",
      "Iteration: 4  Accuracy Score:  0.4\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.2\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.1\n",
      "Iteration: 10  Accuracy Score:  0.2\n",
      "[0.5 0.4 0.1 0.4 0.3 0.3 0.2 0.4 0.1 0.2]\n",
      "\n",
      "---- Word2Vec Results----\n",
      "\n",
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.4\n",
      "Iteration: 4  Accuracy Score:  0.1\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.4\n",
      "Iteration: 8  Accuracy Score:  0.5\n",
      "Iteration: 9  Accuracy Score:  0.4\n",
      "Iteration: 10  Accuracy Score:  0.2\n",
      "Statistics=11.500, p=0.670\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Classification Method\n",
    "\n",
    "# Bow - Tf-Idf\n",
    "print(\"\\n---- Bow - Tf-Idf Results----\\n\")\n",
    "randForestClas = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "accuracyArrayX = TenFoldValidation_BowTfIdf(summarisationData, randForestClas)\n",
    "print(accuracyArrayX)\n",
    "# Word2Vec\n",
    "print(\"\\n---- Word2Vec Results----\\n\")\n",
    "randForestClas = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "accuracyArrayY = TenFoldValidation_Word2Vec(summarisationData, randForestClas)\n",
    "\n",
    "# from scipy.stats import wilcoxon\n",
    "# stat, p = wilcoxon(accuracyArrayX, accuracyArrayY)\n",
    "# print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.6\n",
      "Iteration: 3  Accuracy Score:  0.4\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.5\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.5\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# K-Neighbors Classifier\n",
    "kNeighborsClas = KNeighborsClassifier(n_neighbors=10)\n",
    "TenFoldValidation_BowTfIdf(summarisationData, kNeighborsClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.2\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.5\n",
      "Iteration: 4  Accuracy Score:  0.5\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.5\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.5\n",
      "Iteration: 10  Accuracy Score:  0.2\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# K-Neighbors Classifier\n",
    "kNeighborsClas = KNeighborsClassifier(n_neighbors=2)\n",
    "TenFoldValidation_Word2Vec(summarisationData, kNeighborsClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.5\n",
      "Iteration: 2  Accuracy Score:  0.7\n",
      "Iteration: 3  Accuracy Score:  0.1\n",
      "Iteration: 4  Accuracy Score:  0.4\n",
      "Iteration: 5  Accuracy Score:  0.2\n",
      "Iteration: 6  Accuracy Score:  0.2\n",
      "Iteration: 7  Accuracy Score:  0.2\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM Classifier\n",
    "svmClas = SVC()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, svmClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.2\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.3\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.1\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.1\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM Classifier\n",
    "svmClas = SVC()\n",
    "TenFoldValidation_Word2Vec(summarisationData, svmClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.6\n",
      "Iteration: 2  Accuracy Score:  0.7\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.2\n",
      "Iteration: 7  Accuracy Score:  0.1\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# Gaussian Process Classifier\n",
    "gaussianProcClas = GaussianProcessClassifier()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, gaussianProcClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.2\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.3\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.1\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.1\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# Gaussian Process Classifier\n",
    "gaussianProcClas = GaussianProcessClassifier()\n",
    "TenFoldValidation_Word2Vec(summarisationData, gaussianProcClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.5\n",
      "Iteration: 2  Accuracy Score:  0.5\n",
      "Iteration: 3  Accuracy Score:  0.1\n",
      "Iteration: 4  Accuracy Score:  0.4\n",
      "Iteration: 5  Accuracy Score:  0.4\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.1\n",
      "Iteration: 8  Accuracy Score:  0.2\n",
      "Iteration: 9  Accuracy Score:  0.5\n",
      "Iteration: 10  Accuracy Score:  0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decisionTreeClas = DecisionTreeClassifier()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, decisionTreeClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.6\n",
      "Iteration: 2  Accuracy Score:  0.0\n",
      "Iteration: 3  Accuracy Score:  0.5\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.5\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.4\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.4\n",
      "Iteration: 10  Accuracy Score:  0.4\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decisionTreeClas = DecisionTreeClassifier()\n",
    "TenFoldValidation_Word2Vec(summarisationData, decisionTreeClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.0\n",
      "Iteration: 2  Accuracy Score:  0.5\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.3\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.1\n",
      "Iteration: 7  Accuracy Score:  0.1\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.3\n",
      "Iteration: 10  Accuracy Score:  0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Ada Boost Classifier\n",
    "adaBoostClas = AdaBoostClassifier()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, adaBoostClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.5\n",
      "Iteration: 2  Accuracy Score:  0.1\n",
      "Iteration: 3  Accuracy Score:  0.4\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.5\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.2\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.3\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Classifier\n",
    "decisionTreeClas = DecisionTreeClassifier()\n",
    "TenFoldValidation_Word2Vec(summarisationData, decisionTreeClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.5\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.3\n",
      "Iteration: 5  Accuracy Score:  0.2\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "gaussianNBClas = GaussianNB()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, gaussianNBClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.4\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.2\n",
      "Iteration: 6  Accuracy Score:  0.4\n",
      "Iteration: 7  Accuracy Score:  0.1\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Multinomial Naive Bayes Classifier\n",
    "multiNBClas = MultinomialNB()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, multiNBClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.5\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.2\n",
      "Iteration: 8  Accuracy Score:  0.5\n",
      "Iteration: 9  Accuracy Score:  0.5\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# SGD Classifier\n",
    "sgdClas = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)\n",
    "TenFoldValidation_BowTfIdf(summarisationData, sgdClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.4\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.3\n",
      "Iteration: 4  Accuracy Score:  0.4\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.1\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.1\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# SGD Classifier\n",
    "sgdClas = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)\n",
    "TenFoldValidation_Word2Vec(summarisationData, sgdClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.3\n",
      "Iteration: 2  Accuracy Score:  0.6\n",
      "Iteration: 3  Accuracy Score:  0.2\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.4\n",
      "Iteration: 6  Accuracy Score:  0.3\n",
      "Iteration: 7  Accuracy Score:  0.1\n",
      "Iteration: 8  Accuracy Score:  0.4\n",
      "Iteration: 9  Accuracy Score:  0.2\n",
      "Iteration: 10  Accuracy Score:  0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic Regression\n",
    "logRegClas = LogisticRegression()\n",
    "TenFoldValidation_BowTfIdf(summarisationData, logRegClas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  Accuracy Score:  0.2\n",
      "Iteration: 2  Accuracy Score:  0.3\n",
      "Iteration: 3  Accuracy Score:  0.3\n",
      "Iteration: 4  Accuracy Score:  0.2\n",
      "Iteration: 5  Accuracy Score:  0.3\n",
      "Iteration: 6  Accuracy Score:  0.1\n",
      "Iteration: 7  Accuracy Score:  0.3\n",
      "Iteration: 8  Accuracy Score:  0.3\n",
      "Iteration: 9  Accuracy Score:  0.1\n",
      "Iteration: 10  Accuracy Score:  0.3\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Logistic Regression\n",
    "logRegClas = LogisticRegression()\n",
    "TenFoldValidation_Word2Vec(summarisationData, logRegClas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
